import tensorrt as trt
from tensorrt import IElementWiseLayer, ILayer, INetworkDefinition, IReduceLayer
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit
import torch
from PIL import Image
import onnxruntime as ort
import torch
from trt_backend import BaseEngine

class EngineBuilder:
    """
    Parses an ONNX graph and builds a TensorRT engine from it
    """
    def __init__(self, verbose=False):
        self.trt_logger = trt.Logger(trt.Logger.INFO)
        if verbose:
            self.trt_logger.min_severity = trt.Logger.Severity.VERBOSE
        trt.init_libnvinfer_plugins(self.trt_logger, namespace="")
        self.builder = trt.Builder(self.trt_logger)
        self.config = self.builder.create_builder_config()
        self.config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)
        self.config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED
        self.config.set_flag(trt.BuilderFlag.DISABLE_TIMING_CACHE)
        self.config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)

    # reference : https://github.com/ELS-RD/transformer-deploy/blob/main/src/transformer_deploy/backends/trt_utils.py#L81
    @staticmethod
    def fix_fp16_network(network_definition: INetworkDefinition) -> INetworkDefinition:
        """
        Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated.
        Indeed, FP16 can't store very large and very small numbers like FP32.
        Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference
        and don't hurt performances.
        :param network_definition: graph generated by TensorRT after parsing ONNX file (during the model building)
        :return: patched network definition
        """
        # search for patterns which may overflow in FP16 precision, we force FP32 precisions for those nodes
        for layer_index in range(network_definition.num_layers - 1):
            layer: ILayer = network_definition.get_layer(layer_index)
            next_layer: ILayer = network_definition.get_layer(layer_index + 1)
            # POW operation usually followed by mean reduce
            if layer.type == trt.LayerType.ELEMENTWISE and next_layer.type == trt.LayerType.REDUCE:
                # casting to get access to op attribute
                layer.__class__ = IElementWiseLayer
                next_layer.__class__ = IReduceLayer
                if layer.op == trt.ElementWiseOperation.POW:
                    layer.precision = trt.DataType.FLOAT
                    next_layer.precision = trt.DataType.FLOAT
                layer.set_output_type(index=0, dtype=trt.DataType.FLOAT)
                next_layer.set_output_type(index=0, dtype=trt.DataType.FLOAT)
        return network_definition
    
    def create_network(self, onnx_path):
        network_flags = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        self.network = self.builder.create_network(network_flags)
        self.parser = trt.OnnxParser(self.network, self.trt_logger)

        if not self.parser.parse_from_file(onnx_path):
            print('Failed to parse the ONNX')
            for idx in range(self.parser.num_errors):
                print(self.parser.get_error(idx))
            return
        
    # For open_clip coca model the sequence length can be dynamic during caption generation
    def set_coco_text_encoder_shapes(self, profile, optimal_batch_size, max_batch_size):
        for i in range(self.network.num_inputs):
            input_node = self.network.get_input(i)
            input_name = input_node.name
            profile.set_shape(
                input_name,
                min=(1, 1),
                opt=(optimal_batch_size, 6),
                max=(max_batch_size, 76)
            )
            
    # For open_clip coca model the sequence length is dynamic during caption generation
    def set_coco_text_decoder_shapes(self, profile, optimal_batch_size, max_batch_size):
        for i in range(self.network.num_inputs):
            input_node = self.network.get_input(i)
            input_name = input_node.name
            input_shape = input_node.shape

            if i == 1:
                profile.set_shape(
                    input_name,
                    min=(1, 1) + tuple(input_shape[2:]),
                    opt=(optimal_batch_size, 6) + tuple(input_shape[2:]),
                    max=(max_batch_size, 76) + tuple(input_shape[2:])
                )
            else:
                profile.set_shape(
                    input_name,
                    min=(1,) + tuple(input_shape[1:]),
                    opt=(optimal_batch_size,) + tuple(input_shape[1:]),
                    max=(max_batch_size,) + tuple(input_shape[1:])
                )


            
    def create_engine(self, engine_path, optimal_batch_size=4, max_batch_size=6,fp16=False,cocatxtEncoder=False,cocatxtDecoder = False):
        if fp16 :
            if self.builder.platform_has_fast_fp16:
                print('Building TensorRT engine with FP16 support')
                self.config.set_flag(trt.BuilderFlag.FP16)
                self.network = self.fix_fp16_network(self.network)
            else :
                print("FP16 is not supported natively on this platform/device")

        profile = self.builder.create_optimization_profile()
        
        # Get input name and shape for building optimization profile
        if cocatxtEncoder:
            self.set_coco_text_encoder_shapes(profile, optimal_batch_size, max_batch_size)
        elif cocatxtDecoder:
            self.set_coco_text_decoder_shapes(profile, optimal_batch_size, max_batch_size)
        else:
            for i in range(self.network.num_inputs):
                input_node = self.network.get_input(i)
                input_name = input_node.name
                input_shape = input_node.shape
                profile.set_shape(
                    input_name,
                    min=(1,) + tuple(input_shape[1:]),
                    opt=(optimal_batch_size,) + tuple(input_shape[1:]),
                    max=(max_batch_size,) + tuple(input_shape[1:])
                )

        self.config.add_optimization_profile(profile)
        serialized_engine = self.builder.build_serialized_network(self.network, config=self.config)
        with open(engine_path, 'wb') as f:
            f.write(serialized_engine)



